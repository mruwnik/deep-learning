{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf07c4c0-1683-4b4d-a020-22381250220b",
   "metadata": {},
   "source": [
    "# Vanilla Neural Network\n",
    "\n",
    "This does what it says it does. Implements a basic NN with sigmoid activations and a L2 cost function. Mostly based on the equations from [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/chap2.html#the_four_fundamental_equations_behind_backpropagation) and [my notes](https://ahiru.pl/notes/backpropagation/).\n",
    "\n",
    "PyTorch is used here mainly as a numpy wrapper, seeing as batch handling is done manually. That being said, it learns MNIST digits, which is pretty much all this was supposed to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac212bfe-98f4-4211-954e-b9197e1fc613",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import fastbook\n",
    "from torch import tensor\n",
    "\n",
    "import mnist\n",
    "from monitoring import graph_updater\n",
    "from math_funcs import is_correct, L2, Sigmoid\n",
    "\n",
    "\n",
    "fastbook.setup_book()     \n",
    "\n",
    "training_set, testing_set, validation_set = mnist.datasets(0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c566f3a-ca52-4386-a55c-cf29e23e96cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self, layers, nonlinearity=Sigmoid(), cost_func=L2()):\n",
    "        # This assumes that each layer will use the same nonlinearity. Seems\n",
    "        # like a good enough heuristic for now\n",
    "        self.nonlinearity = nonlinearity\n",
    "        self.cost_func = cost_func\n",
    "        self.w = [torch.rand(n_out, n_in) / torch.sqrt(tensor(n_in)) for n_in, n_out in zip(layers, layers[1:])]\n",
    "        self.b = [torch.rand(n_out, 1) for n_out in layers[1:]]\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Do a forward pass and return all the activations for each layer.\n",
    "        \n",
    "        The inputs will be the first element of the activations.\n",
    "        The actual output of the network is the last element of the resulting list.\n",
    "        \"\"\"\n",
    "        a = inputs\n",
    "        activations = [a]\n",
    "        for w, b in zip(self.w, self.b):\n",
    "            a = self.nonlinearity(w @ a + b)\n",
    "            activations.append(a)\n",
    "        return activations\n",
    "         \n",
    "    def step(self, inputs):\n",
    "        \"\"\"\"Get the outputs from running the given `inputs` through the network.\"\"\"\n",
    "        return self.forward(inputs)[-1]\n",
    "    \n",
    "    def gradient(self, activations, expected):\n",
    "        \"\"\"Calculate the gradient(s).\n",
    "        \n",
    "        :param list activations: a list of per layer activations from the forward pass\n",
    "        :param tensor expected: the expected output of the network, i.e. the label for\n",
    "                                the inputs\n",
    "        :returns: a list of per layer (dw, db) gradients, from the first layer to the last\n",
    "        \n",
    "        Seeing as it uses pytorch, either single items, or tensors of items can be\n",
    "        provided.\n",
    "        \n",
    "        This is based on http://neuralnetworksanddeeplearning.com/chap2.html#the_four_fundamental_equations_behind_backpropagation\n",
    "        and https://ahiru.pl/notes/backpropagation/ \n",
    "        \"\"\"\n",
    "        layer_gradients = []\n",
    "       \n",
    "        # Calculate the gradients for the final layer, which is\n",
    "        # different from the rest in that it's based directly on\n",
    "        # The cost function, as opposed to the error of a later\n",
    "        # layer\n",
    "        in_ = activations[-2]\n",
    "        out = activations[-1]\n",
    "\n",
    "        nabla_C = self.cost_func.deriv(out, expected)\n",
    "        sigma_L = self.nonlinearity.deriv(out)\n",
    "        delta_L = nabla_C * self.nonlinearity.deriv(out)\n",
    "\n",
    "        prev_delta = delta_L\n",
    "        \n",
    "        dw = prev_delta @ torch.transpose(in_, 0, 1) \n",
    "        db = prev_delta \n",
    "        layer_gradients.append([dw, db])\n",
    "        \n",
    "        # Now for each layer, starting from the last-but-one and going\n",
    "        # to the first one, calculate the gradients of the weights and\n",
    "        # biases\n",
    "        for l in range(len(self.w) - 2, -1, -1):\n",
    "            in_ = activations[l]\n",
    "            out = activations[l+1]\n",
    "            \n",
    "            delta_l = (torch.transpose(self.w[l+1], 0, 1) @ prev_delta) * self.nonlinearity.deriv(out)\n",
    "            prev_delta = delta_l\n",
    "            \n",
    "            dw = delta_l @ torch.transpose(in_, 0, 1)\n",
    "            db = delta_l\n",
    "            \n",
    "            layer_gradients.append([dw, db])\n",
    "        \n",
    "        # The `layer_gradients` list is from the last to first layer,\n",
    "        # so reverse it\n",
    "        return list(reversed(layer_gradients))\n",
    "    \n",
    "    def update(self, gradient, lr):\n",
    "        # For each layer of the gradient list, update the\n",
    "        # weights and biases for that layer\n",
    "        for i, (w, b) in enumerate(gradient):\n",
    "            self.w[i] -= lr * w\n",
    "            self.b[i] -= lr * b\n",
    "            \n",
    "    def cost(self, expected, output):\n",
    "        \"\"\"Calculate the cost between the label and what the network predicted.\"\"\"\n",
    "        return self.cost_func(expected, output)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56a6571d-afca-4811-90c8-7fe7212d9587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The gradients are collected per input, but they should be applied per batch. These\n",
    "# functions mangle the gradient lists from [<grad 1>, <grad 2>, ...] to \n",
    "# [<mean grad for layer 1>, <mean grad for layer 2>, ...]\n",
    "\n",
    "def merge_layer_grads(grads):\n",
    "    return [torch.stack(i).mean(0) for i in zip(*grads)]\n",
    "    \n",
    "\n",
    "def merge_grads(grads):\n",
    "    return [merge_layer_grads(l) for l in zip(*grads)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "221de779-4e9d-46a6-a6f5-ac521f3d8848",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(network, dataset, lr):\n",
    "    accuracy = []\n",
    "    costs = []\n",
    "    \n",
    "    for batch in iter(dataset):\n",
    "        grads = []\n",
    "        for xs, expected in zip(*batch):\n",
    "            activations = network.forward(xs)\n",
    "            output = activations[-1]\n",
    "            grad = network.gradient(activations, expected)\n",
    "            grads.append(grad)\n",
    "            \n",
    "            cost = network.cost(expected, output)\n",
    "            \n",
    "            accuracy.append(is_correct(expected, output))\n",
    "            costs.append(cost)\n",
    "         \n",
    "        grad = merge_grads(grads)\n",
    "        network.update(grad, lr)\n",
    "\n",
    "    return tensor(accuracy).float().mean(), torch.stack(costs).mean()\n",
    "\n",
    "\n",
    "def train(network, dataset, epochs, lr, show_progress=True):\n",
    "    if show_progress:\n",
    "        add_datapoint = graph_updater()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        start = datetime.now()\n",
    "        \n",
    "        avg_accuracy, avg_cost = run_epoch(network, dataset, lr)\n",
    "        \n",
    "        if show_progress and (not epoch % 1):\n",
    "            print(f'epoch {epoch}: accuracy={round(avg_accuracy.item(), 3)}, cost={round(avg_cost.item(), 3)}, time: {datetime.now() - start}')\n",
    "            add_datapoint(avg_accuracy)\n",
    "    \n",
    "    return avg_accuracy, avg_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2485578e-c56f-4263-b90d-da629ccb9440",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = Network((28*28, 50, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f77eb00b-a945-489f-a4a9-d17627ffee90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.8565), tensor(0.1154))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train(n, training_set, epochs=7, lr=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36530004-0a00-4e5c-9d80-3abe2a432f07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
